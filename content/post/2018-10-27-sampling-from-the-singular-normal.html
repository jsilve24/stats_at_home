---
title: Sampling from the Singular Normal
author: Justin Silverman
date: '2018-10-27'
slug: sampling-from-the-singular-normal
categories: []
tags:
  - R
  - Sampling
type: post
description: A follow up to post on sampling from multivariate normal 
  describing the case when the covariance or precision matricies are singular 
  (not positive definite). In these cases the Eigen decomposition provides a means
  of calculating the matrix square root. 
---



<p>Following up the previous post on <a href="http://www.statsathome.com/2018/10/19/sampling-from-multivariate-normal-precision-and-covariance-parameterizations/">sampling from the multivariate normal</a>, I decided to describe in more detail the situation where the covariance matrix or precision matrix is singular (e.g., it is not positive definite). A normal distribution with such a singular covariance/precision matrix is referred to as a singular normal distribution. Here is 100 samples from a two dimensional example:</p>
<p><img src="/post/2018-10-27-sampling-from-the-singular-normal_files/figure-html/unnamed-chunk-2-1.png" width="384" style="display: block; margin: auto;" /> Notice that a singular normal essentially has less dimensions (in this case 1 dimension) than the dimension of the random variable (in this case 2 dimensions).</p>
<p>When working with singular normals lots of problems can arise, the most common is issues relating to sampling from these distributions. In these situations the Cholesky decomposition of the covariance / precision matrix will fail. If you are using R you may end up with an error like the following:</p>
<pre class="r"><code># Create singular covariance matrix
(Sigma &lt;- crossprod(matrix(rnorm(3),1,3)))</code></pre>
<pre><code>##           [,1]       [,2]       [,3]
## [1,]  1.932865  1.2123852 -1.5026330
## [2,]  1.212385  0.7604659 -0.9425232
## [3,] -1.502633 -0.9425232  1.1681654</code></pre>
<pre class="r"><code># This should cause an error - use safely (from purrr) so that site still renders
safely(chol)(Sigma)</code></pre>
<pre><code>## $result
## NULL
## 
## $error
## &lt;simpleError in chol.default(...): the leading minor of order 2 is not positive definite&gt;</code></pre>
<p>In fact even inverting this covariance matrix will run into issues as the inverse is not well defined.</p>
<pre class="r"><code>safely(solve)(Sigma)</code></pre>
<pre><code>## $result
## NULL
## 
## $error
## &lt;simpleError in solve.default(...): system is computationally singular: reciprocal condition number = 6.86973e-18&gt;</code></pre>
<p>One solution to this problem is given by the Eigen decomposition which will not only allow us to sample from such a singular covariance matrix but it will also allow us to detect when a covariance matrix is singular.</p>
<p><strong>A Key Point:</strong> The Cholesky decomposition is typically much faster than the Eigen decomposition. Therefore, if you know your covariance / precision matrix is not singular the Cholesky decomposition is typically a better choice. I described the use of the Cholesky distribution for this purpose in detailed <a href="http://www.statsathome.com/2018/10/19/sampling-from-multivariate-normal-precision-and-covariance-parameterizations/">here</a>.</p>
<div id="background-the-eigen-decomposition" class="section level1">
<h1>Background: The Eigen Decomposition</h1>
<p>Any square matrix with real elements <span class="math inline">\(\Sigma\)</span><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> can be decomposed as <span class="math display">\[\Sigma = V_\Sigma D_\Sigma V_\Sigma^T\]</span> where <span class="math inline">\(V\)</span> is a matrix of orthonormal column vectors (called the Eigenvectors) and <span class="math inline">\(D\)</span> is a diagonal matrix with diagonal elements called Eigenvalues that are decreasing (that is <span class="math inline">\(D_{ii} \geq D_{(i+1)(i+1)}\)</span>).</p>
<p>Lots of <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">great stuff has been written about the Eigen decomposition</a>, rather than rehash these topics here I will instead just point out that the Eigen decomposition can be used to find a matrix square root (just like the Cholesky). Remember that the square root of a matrix <span class="math inline">\(\Sigma\)</span> is defined by the relation <span class="math inline">\(\Sigma = \Sigma^{\frac{1}{2}}\left(\Sigma^{\frac{1}{2}}\right)^T\)</span>. For the Eigen decomposition we can write <span class="math display">\[\Sigma = V_\Sigma D_\Sigma^{\frac{1}{2}}\left(V_\Sigma D_\Sigma^{\frac{1}{2}}\right)^T\]</span> where <span class="math inline">\(D^{\frac{1}{2}}\)</span> is essentially the same and <span class="math inline">\(D\)</span> but where the diagonal is given by the square root of the Eigenvalues (rather than by the Eigenvalues). This shows that <span class="math inline">\(VD^{\frac{1}{2}}\)</span> is a matrix square root and suggests we can use this in place of the Cholesky factor in sampling from the multivariate normal.</p>
<p>The Eigen decomposition is also rank revealing (it will tell you if your covariance/precision matrix is singular and if so how singular it is). Cutting to the chase, a symmetric positive definite matrix <span class="math inline">\(X\)</span> is a symmetric matrix where all the Eigenvalues are positive. If all the Eigenvalues are negative its symmetric negative-definite. If a <span class="math inline">\(p\times p\)</span> symmetric matrix <span class="math inline">\(X\)</span> has <span class="math inline">\(k &lt; p\)</span> positive Eigenvalues and <span class="math inline">\(p-k\)</span> zero Eigenvalues then we say <span class="math inline">\(X\)</span> is symmetric positive <em>semi</em>-definite (<em>e.g.</em>, singular and of rank <span class="math inline">\(k\)</span>). If some Eigenvalues are positive and some are negative and you expected <span class="math inline">\(X\)</span> to be a covariance / precision matrixâ€¦ something has gone very wrong<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.</p>
<p>So lets take a look at the Eigen decomposition of the matrix <code>Sigma</code> that was giving us trouble above.</p>
<pre class="r"><code>(es &lt;- eigen(Sigma))</code></pre>
<pre><code>## eigen() decomposition
## $values
## [1] 3.861496e+00 2.220446e-15 0.000000e+00
## 
## $vectors
##            [,1]       [,2]      [,3]
## [1,] -0.7074943  0.7067191 0.0000000
## [2,] -0.4437742 -0.4442610 0.7782651
## [3,]  0.5500148  0.5506181 0.6279358</code></pre>
<p>Note that only one of the Eigenvalues is positive and the other two are zero<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>; this means that <code>Sigma</code> is positive-semidefinite (singular of rank 1).</p>
</div>
<div id="sampling-from-the-singular-normal" class="section level1">
<h1>Sampling from the Singular Normal</h1>
<p>Just as I introduced regarding <a href="http://www.statsathome.com/2018/10/19/sampling-from-multivariate-normal-precision-and-covariance-parameterizations/">sampling from the non-singular multivariate normal</a>, here we are going to use the non-centered parameterization to generate singular normal random variables from univariate standard normal (mean zero variance 1) draws. To sample <span class="math inline">\(n\)</span> draws form a <span class="math inline">\(p\)</span> dimensional singular normal distribution let us introduce the <span class="math inline">\(p \times n\)</span> matrix <span class="math inline">\(Z\)</span> with elements <span class="math inline">\(Z_{ij} \sim N(0, 1)\)</span>.</p>
</div>
<div id="covariance-parameterization" class="section level1">
<h1>Covariance Parameterization</h1>
<p>To sample <span class="math inline">\(X \sim N(\mu, \Sigma)\)</span> where <span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(p\times p\)</span> potentially singular covariance matrix, we have the following non-centered relationship <span class="math display">\[X = \mu + V_\Sigma D^{\frac{1}{2}}_\Sigma Z\]</span></p>
<p>Note that if <span class="math inline">\(\Sigma\)</span> is of rank <span class="math inline">\(k&lt; p\)</span>, there is some redundancy here: <span class="math inline">\(Z\)</span> can actually be of dimension <span class="math inline">\(k \times n\)</span> and we can simply take the first <span class="math inline">\(k\)</span> columns of <span class="math inline">\(V_\Sigma\)</span> and <span class="math inline">\(D^{\frac{1}{2}}_\Sigma\)</span> and we will still get the same answer.</p>
</div>
<div id="precision-parameterization" class="section level1">
<h1>Precision Parameterization</h1>
<p>To sample <span class="math inline">\(X \sim N(\mu, \Omega)\)</span> where <span class="math inline">\(\Omega\)</span> is a <span class="math inline">\(p\times p\)</span> potentially singular precision matrix, we have the following non-centered relationship <span class="math display">\[X = \mu + V_\Omega D^{-\frac{1}{2}}_\Omega Z\]</span> Here the only difference compared to the covariance parameterization is that the elements of <span class="math inline">\(D^{-\frac{1}{2}}_\Omega\)</span> are given by <span class="math inline">\(1/\sqrt{D_{ii}}\)</span>.</p>
<p>Again, note that if <span class="math inline">\(\Omega\)</span> is of rank <span class="math inline">\(k&lt; p\)</span>, there is some redundancy here: <span class="math inline">\(Z\)</span> can actually be of dimension <span class="math inline">\(k \times n\)</span> and we can simply take the first <span class="math inline">\(k\)</span> columns of <span class="math inline">\(V_\Omega\)</span> and <span class="math inline">\(D^{\frac{1}{2}}_\Omega\)</span> and we will still get the same answer.</p>
</div>
<div id="code" class="section level1">
<h1>Code</h1>
<p>Below I demonstrate code designed to sample from the singular multivariate normal in either parameterization. In particular, note that these functions make use of the truncation I mentioned above that reduces the redundancy in the non-centered distribution for the singular normal case.</p>
<p>Also notice that these functions can be used to sample from non-singular multivariate normal distributions as well. In this way these functions are more general than <a href="http://www.statsathome.com/2018/10/19/sampling-from-multivariate-normal-precision-and-covariance-parameterizations/">those given previously</a> but they are typically slower because they use the Eigen decomposition rather than the Cholesky.</p>
<pre class="r"><code>#&#39; Truncated Eigen Square Root (or inverse square root) 
#&#39;
#&#39; Designed for Covaraice or Precision matricies - throws error if 
#&#39; there is any large negative Eigenvalues. 
#&#39;
#&#39; @param Sigma p x p matrix to compute truncated matrix square root
#&#39; @param inv (boolean) whether to compute inverse of square root
#&#39; @return p x k matrix (where k is rank of Sigma) 
trunc_eigen_sqrt &lt;- function(Sigma, inv){
  es &lt;- eigen(Sigma)
  
  # Small negative eigenvalues can occur just due to numerical error and should
  # be set back to zero. 
  es$values[(es$values &lt; 1e-12) &amp; (es$values &gt; -1e-12)] &lt;- 0
  
  # If any eigen values are large negative throw error (something wrong)
  if (any(es$values &lt; -1e-12)) stop(&quot;Non-trivial negative eigenvalues present&quot;)
  
  # calculate square root and reveal rank (k)
  k &lt;- sum(es$values &gt; 0)
  if (!inv){
    L &lt;- es$vectors %*% diag(sqrt(es$values))  
  } else if (inv) {
     L &lt;- es$vectors %*% diag(1/sqrt(es$values))  
  }
  return(L[,1:k, drop=F])
}

#&#39; Covariance parameterization (Eigen decomposition)
#&#39; @param n number of samples to draw
#&#39; @param mu p-vector mean
#&#39; @param Sigma covariance matrix (p x p)
#&#39; @return matrix of dimension p x n of samples
rMVNormC_eigen &lt;- function(n, mu, Sigma){
  p &lt;- length(mu)
  L &lt;- trunc_eigen_sqrt(Sigma, inv=FALSE)
  k &lt;- ncol(L)
  Z &lt;- matrix(rnorm(k*n), k, n)
  X &lt;- L%*%Z
  X &lt;- sweep(X, 1, mu, FUN=`+`)
}

#&#39; Precision parameterization (Eigen decomposition)
#&#39; @param n number of samples to draw
#&#39; @param mu p-vector mean
#&#39; @param Sigma precision matrix (p x p)
#&#39; @return matrix of dimension p x n of samples
rMVNormP_eigen &lt;- function(n, mu, Sigma){
  p &lt;- length(mu)
  L &lt;- trunc_eigen_sqrt(Sigma, inv=TRUE) # only difference vs. cov parameterization
  k &lt;- ncol(L)
  Z &lt;- matrix(rnorm(k*n), k, n)
  X &lt;- L%*%Z
  X &lt;- sweep(X, 1, mu, FUN=`+`)
}</code></pre>
<p>Now we just check that the mean and covariance of each function matches what it should be (using the same <code>Sigma</code> we defined above).</p>
<pre class="r"><code>n &lt;- 10000
mu &lt;- 1:3
Omega &lt;- MASS::ginv(Sigma) # use pseudoinverse 
x1 &lt;- rMVNormC_eigen(n, mu, Sigma)
x2 &lt;- rMVNormP_eigen(n, mu, Omega)

# Create function that tests for equality with high tolerance due to 
# random number generation
weak_equal &lt;- function(x, y) all.equal(x, y, tolerance=.05)

# check row means match up with mu and agree with eachother
weak_equal(rowMeans(x1), mu) &amp; weak_equal(rowMeans(x2), mu)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code># check empirical covariances agree
weak_equal(var(t(x1)), var(t(x2)))</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Note that we used the pseudo-inverse of <code>Sigma</code> above to create a rank-deficient precision matrix (<code>Omega</code>) that should correspond to the same distribution as <code>Sigma</code>.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note this is more general than for just symmetric positive definite matrices as in the definition of the Cholesky decomposition <a href="http://www.statsathome.com/2018/10/19/sampling-from-multivariate-normal-precision-and-covariance-parameterizations/">introduced previously</a><a href="#fnref1">â†©</a></p></li>
<li id="fn2"><p>I only really see this when using Laplace approximations and when the optimization ended at a saddle point rather than at the <em>Maximum a Posteriori</em> estimate.<a href="#fnref2">â†©</a></p></li>
<li id="fn3"><p>any slight non-zero number in either of the last two Eigenvalues is just due to numerical errors in the calculation of the Eigen decomposition<a href="#fnref3">â†©</a></p></li>
</ol>
</div>
