---
title: Measure Theory Made Ridiculously Simple
author: Justin Silverman
date: '2017-06-26'
slug: measure-theory-made-ridiculously-simple
categories: []
tags: 
  - Made Ridiculously Simple
description: Measure theory is actually really simple. Here are some core concepts of measure theory, introduced in a ridiculously simple way. 
draft: no
keywords:
  - key
  - words
topics: topic 1
type: post
bibliography: measure_theory_made_simple.bib
---

<!-- BLOGDOWN-HEAD -->
<!-- /BLOGDOWN-HEAD -->

<!-- BLOGDOWN-BODY-BEFORE -->
<!-- /BLOGDOWN-BODY-BEFORE -->
<p>During my first few years of medical school I became a big fan of the <em>[Subject] Made Rediculously Simple</em> book series (as in <a href="https://www.amazon.com/Clinical-Microbiology-Made-Ridiculously-Simple/dp/1935660152"><em>Clinical Microbiology Made Rediculously Simple</em></a>). I found that the authors did a great job of simplifying the subject matter, sometimes to the point of absurdity, while getting the core concepts across in a memorable way. For some time now I have wished that similar tools were available for mathematics. While I may not have the same artistry or comical flare that those authors have, here I attempt to take the <em>Made Rediculously Simple</em> flare to explain some of the core concepts of measure theory. This post is designed for those who have a background in basic calculus and recognize that measure theory plays some important (although perhaps cryptic) role in modern probability and statistics.</p>
<p>I have heard a number of people say that they have a hard time understanding measure theory. I think the confusion is very understandable, the discussion of measures are often complicated by the need to introduce the formalism of <span class="math inline">\(\sigma\)</span>-algebras and the Borel <span class="math inline">\(\sigma\)</span>-field and thus students can often feel overwhelmed by the sheer number of new abstract definitions and concepts that must be understood all together. I am very thankful that I have had some really good mentors who have distilled the concepts of measure theory to a point that seems very intuitive to me. In this post I will try to convey a very rough but intuitive introduction to measure theory in the hopes that it may serve as a less confusing gateway to the subject. Please note that I have sacrificed some mathematical rigor and correctness in my attempt to divorce the concepts of measures from set theory. Instead I try to convey an intuitive notion of some concepts in measure theory and then explain why the set theory definitions are needed at the end.</p>
<div id="why-should-i-care" class="section level1">
<h1>Why should I care?</h1>
<p>The first question I wanted to just briefly touch on is: Why should I care about measure theory?</p>
<div id="the-theoretical-answer" class="section level3">
<h3>The Theoretical Answer</h3>
<p>The theoretical answer is that measure theory literally underlies the entire notion of random-variables, probability, and statistics. In fact, studying measure theory has helped me conceptualize the question: Why should I bother using the concept of probability at all?</p>
</div>
<div id="the-applied-answer" class="section level3">
<h3>The Applied Answer</h3>
<p>That said, I realize this theoretical answer may not appeal to the more applied person who will want to know how these concepts can be used in statistical practice. To such a person I would admit that (currently) the applications of measure theory in statistics seems largely relegated to researchers studying new types of data and doing methodological research; however, there is a growing number of tools that actually use measure theory concepts directly in the analysis of real-world data sets. For example, in the field of compositional data analysis, changing the reference measure of the simplex has turned out to be a powerful method to up or down-weight certain variables in an analysis <span class="citation">(Egozcue and Pawlowsky-Glahn 2016)</span>. Another cool example is the use of measure theory in the study of Bayes Linear spaces which give a vector space structure to the space of probability densities <span class="citation">(van den Boogaart, Egozcue, and Pawlowsky-Glahn 2014)</span>.</p>
<p>As a final note, I would point out that re-expressing a density with regards to a different reference measure can make working with some fairly intractable densities possible. For example, the <a href="https://en.wikipedia.org/wiki/Logit-normal_distribution#Multivariate_generalization">Logistic-Normal distribution</a> is a fairly difficult distribution to work with directly (e.g., its mean and variance have no analytical closed form with respect to the Lebesgue measure). However, reparameterization of this distribution in terms of log-ratios converts this nasty distribution into the multivariate normal distribution. In <span class="citation">Mateu-Figueras, Pawlowsky-Glahn, and Egozcue (2013)</span>, the authors show that this reparameterization is actually equivalent to changing the reference measure from the standard <a href="https://en.wikipedia.org/wiki/Lebesgue_measure">Lebesgue Measure</a> to the Aitchison Measure. In essence, they show that changing the reference measure can make a seemingly intractable density tractable.</p>
</div>
</div>
<div id="the-meat-of-this-post-start-here-if-short-on-time" class="section level1">
<h1>The meat of this post (start here if short on time)</h1>
<p>I would say that many concepts in measure theory can be seen as a generalization of the following image:</p>
<center>
<img src="/img/2017-06-26-measure-theory-made-rediculously-simple/measure_simple.png" alt="Conceptual Image" style="width:8in">
</center>
<p><strong>Essentially a probability measure is a generalization of a volume element.</strong> <span class="math inline">\(\mu(x)\)</span> is a probability measure, essentially a function that takes in an interval or a set of points and outputs a positive real value representing the “area/volume” (or amount) of probability in the specified region. As a point of terminology, we refer to <span class="math inline">\(\mu(x)\)</span> as the <strong>probability measure</strong>, <span class="math inline">\(\lambda(x)\)</span> as the <strong>reference measure</strong>, and <span class="math inline">\(f(x)\)</span> as the probability density function. Note that we often take <span class="math inline">\(\lambda(x)\)</span> as the <a href="https://en.wikipedia.org/wiki/Lebesgue_measure">Lebesgue Measure</a> which is essentially just a uniform function over the sample space (i.e., the Lebesgue measure is what you would probably think to do by default before you even learned about measure theory).</p>
<p><strong>The probability measure <span class="math inline">\(\mu(x)\)</span> is really the crucial probabilistic object we use for modeling.</strong> It is the probability measure that tells us the probability of an event <span class="math inline">\(x\)</span>. The reference measure <span class="math inline">\(\lambda(x)\)</span> is essentially just a meter-stick that allows us to express the probability measure as a simple function <span class="math inline">\(f(x)\)</span>. That is, we represent the probability measure <span class="math inline">\(\mu(x)\)</span> as <span class="math inline">\(f(x)\)</span> by comparing the probability measure to some specified reference measure <span class="math inline">\(\lambda(x)\)</span>. This is essentially the intuition that is given by the <a href="https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem">Radon-Nikodym derivative</a> <span class="math display">\[f(x) = \frac{d\mu(x)}{d\lambda(x)}\]</span> or equivalently (height = area/width). Note that we can also represent the same idea by <span class="math display">\[\mu(A) = \int_{A\in X}f(x)d\lambda(x)\]</span> where <span class="math inline">\(\mu(A)\)</span> is the sum of the probability of events in the set <span class="math inline">\(A\)</span> which is itself a subset of the entire sample space <span class="math inline">\(X\)</span>. Note that when <span class="math inline">\(A=X\)</span> then the integral must equal 1 by definition of probability.</p>
<div id="reexpressing-a-density-with-respect-to-a-different-reference-measure" class="section level3">
<h3>Reexpressing a density with respect to a different reference measure</h3>
<p>Just to point out how much basic calculus and algebra can extend our intuition for measure theory, I will briefly discuss how you can re-express a given density <span class="math inline">\(f(x)\)</span> with respect to a different reference measure. Changing the reference measure is kind of like stretching or squishing the density around on the <span class="math inline">\(x\)</span> axis. As you stretch or squish the density you keep the total volume constant even though the height of the density <span class="math inline">\(f(x)\)</span> changes. That is, by changing the reference measure (stretching or squishing) you can change the functional form of the density <span class="math inline">\(f(x)\)</span> while keeping the core concept (the association of probability to events, which is given by the probability measure <span class="math inline">\(\mu(x)\)</span>) the same. Note also the Radon-Nikodym Chain Rule shows us how to re-express a density with respect to a different measure (note this is essentially basic calculus and algebra). Lets say you want to re-express the density of <span class="math inline">\(f(x)\)</span> with respect to a compatible measure <span class="math inline">\(\omega(x)\)</span>, then we have <span class="math display">\[ f_\omega(x) = \frac{d\mu(x)}{d\lambda(x)}\frac{d\lambda(x)}{d\omega(x)} = \frac{d\mu(x)}{d\omega(x)}. \]</span> Thus while <span class="math inline">\(f(x)\)</span> may be very difficult to work with, it is possible that <span class="math inline">\(f_\omega(x)\)</span> (which represents the same probabilistic object) is much easier to work with (think the Logistic Normal and the link to the Multivariate Normal as I mentioned briefly above).</p>
</div>
</div>
<div id="so-why-all-the-set-theory" class="section level1">
<h1>So why all the set theory?</h1>
<p>Hopefully this description of measure theory is much more approachable than those that start by defining <span class="math inline">\(\sigma\)</span>-algebras and the Borel <span class="math inline">\(\sigma\)</span>-field. However, the question must be broached, why then the need for this complicated set theory? Here I have made use of our familiarity with Real space to try to get the core concepts of measure theory across, but measure theory is much more powerful and general than just measures of probability over Real space. For example, imagine defining a probability distribution over permutations of objects. You can imagine that the definition of area/volume and width, are likely going to need to need to be generalized to this much less intuitive space of permutations. Essentially the language of <span class="math inline">\(\sigma\)</span>-algebras/fields gives the necessary formalism to generalize these definitions. The Borel <span class="math inline">\(\sigma\)</span>-field is simply a special definition for <span class="math inline">\(\sigma\)</span>-fields over open intervals in Real space. We need this formalism because it can be difficult to work with the infiniteness of Real space.</p>
</div>
<div id="more-resources-to-learn-from" class="section level1">
<h1>More Resources to Learn From</h1>
<p>For those that want to learn more about measure theory, I would recommend starting with the excellent 4 page tutorial <a href="https://moodle.lmu.de/mod/resource/view.php?id=10032"><em>A Measure Theory Tutorial (Measure Theory for Dummies)</em> by Maya R. Gupta</a>. This tutorial is a nice bridge between the almost reckless disregard for formality I present and the crucial definitions and terminology that are the foundations of measure theory. I encourage readers to pay particular attention to the section entitled “The truth about <em>Random Variables</em>”. At the end of this tutorial she gives a number of recommendations for introductory texts on the subject.</p>
<div id="one-final-point-warning-abstractitude-sickness-or-the-flatlanders-glimpse-ahead" class="section level3">
<h3>One Final Point (Warning: <a href="https://mathwithbaddrawings.com/2017/01/25/a-guide-to-mathematical-emotions/">Abstractitude Sickness or The Flatlander’s Glimpse</a> Ahead!)</h3>
<p>Just as another cool connection to draw, note the very close (but not exact) <a href="https://en.wikipedia.org/wiki/Differential_form#Relation_with_measures">relation between differential forms and measures</a> and the connection between differential forms, changes of measures (e.g., the chain rule for the Radon-Nikodym derivative), and the <a href="http://www.math.uah.edu/stat/dist/Transformations.html">transformation of variables</a> formula that is taught in first year statistics courses!</p>
<p>To spark some ideas: <span class="math display">\[ g(y) = f(x)\left\vert\frac{dx}{dy} \right\vert \]</span> which implies that <span class="math inline">\(g(y) \vert dy\vert = f(x) \vert dx \vert = d\mu(x)\)</span>. I believe that here <span class="math inline">\(\vert dx\vert\)</span> and <span class="math inline">\(\vert dy \vert\)</span> can be taken as the Lebesgue measures for the sample spaces <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> respectively (<span class="math inline">\(d\lambda(x)\)</span> and <span class="math inline">\(d\lambda(y)\)</span>). From the above discussion we could also write <span class="math display">\[ g(y) =  \frac{d\mu(x)}{d\lambda(x)}\frac{d\lambda(x)}{d\lambda(y)}\]</span> which is essentially identical to what the chain rule for the Radon-Nikodym derivative.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-egozcue2016">
<p>Egozcue, J. J., and V. Pawlowsky-Glahn. 2016. “Changing the Reference Measure in the Simlex and Its Weightings Effects.” Journal Article. <em>Austrian Journal of Statistics</em> 45 (4): 25–44.</p>
</div>
<div id="ref-mateufigueras2013">
<p>Mateu-Figueras, G., V. Pawlowsky-Glahn, and J. J. Egozcue. 2013. “The Normal Distribution in Some Constrained Sample Spaces.” Journal Article. <em>SORT</em> 37 (1): 29–56.</p>
</div>
<div id="ref-vandenboogart2010">
<p>van den Boogaart, K. V., J. J. Egozcue, and V. Pawlowsky-Glahn. 2014. “Bayes Hilbert Spaces.” Journal Article. <em>Australian and New Zealand Journal of Statistics</em> 56 (2): 171–94.</p>
</div>
</div>
</div>
